{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from sklearn.metrics.pairwise import  laplacian_kernel\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy.integrate import quad\n",
    "from scipy.interpolate import interp1d\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from WHIDataModule import *\n",
    "from estimators import *\n",
    "from utils import *\n",
    "from mmr_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_mmr(df_pca, train_index, mmr_index, mmr_keys, jD):\n",
    "\n",
    "    df_train = df_pca.iloc[train_index, :].copy()\n",
    "    df_mmr = df_pca.iloc[mmr_index, :].copy()\n",
    "\n",
    "    # fit a model for the selection score P(S=1|X) and clip the population based on it\n",
    "    X, y = df_train[jD[\"cov_list\"]], df_train[\"S\"]\n",
    "\n",
    "    logreg = LogisticRegressionCV(cv=5, random_state=42, penalty='l2', solver='lbfgs')\n",
    "    logreg.fit(X, y)\n",
    "\n",
    "    df_mmr['P(S=1|X)'] = logreg.predict_proba(df_mmr[jD[\"cov_list\"]])[:,1]\n",
    "    df_mmr = df_mmr.loc[(df_mmr['P(S=1|X)'] > 0.05) \\\n",
    "                        & (df_mmr['P(S=1|X)']< 0.95)].reset_index(drop=True)\n",
    "\n",
    "    # fit a model for the propensity scores P(A=1|X,S) and clip the population based on them\n",
    "    for s in range(2):\n",
    "        X = df_train.query(f\"S=={s}\")[jD[\"cov_list\"]]\n",
    "        y = df_train.query(f\"S=={s}\")[\"A\"]\n",
    "\n",
    "        logreg = LogisticRegressionCV(cv=5, random_state=42, penalty='l2', solver='lbfgs')\n",
    "        logreg.fit(X, y)\n",
    "\n",
    "        df_mmr.loc[df_pca['S']==s, 'P(A=1|X,S)'] = logreg.predict_proba(df_mmr[jD[\"cov_list\"]])[:,1]\n",
    "\n",
    "    df_mmr = df_mmr.loc[(df_mmr['P(A=1|X,S)'] > 0.05) \\\n",
    "                        & (df_mmr['P(A=1|X,S)']< 0.95)].reset_index(drop=True)\n",
    "\n",
    "    # fit response surface signals with the imputed data\n",
    "    mu_regressor = {}\n",
    "    for s in range(2):\n",
    "        for a in range(2):\n",
    "            mu_regressor[f'S{s}_A{a}'] =\\\n",
    "            mu_est_baseline(df_train.query(f'S=={s} & A=={a}').copy(), 'T', jD[\"cov_list\"], model_name='linear')\n",
    "\n",
    "        df_mmr.loc[df_mmr[\"S\"]==s, 'mu(Y|X,S,A=0)'] = mu_regressor[f'S{s}_A0'].predict(df_mmr.loc[df_mmr[\"S\"]==s, jD[\"cov_list\"]])\n",
    "        df_mmr.loc[df_mmr[\"S\"]==s, 'mu(Y|X,S,A=1)'] = mu_regressor[f'S{s}_A1'].predict(df_mmr.loc[df_mmr[\"S\"]==s, jD[\"cov_list\"]])\n",
    "\n",
    "    # fit the survival curves\n",
    "    Gb_C, Fb_Y = est_surv_whi(df_train, 'coxph', jD[\"cov_list\"], downsample=10)\n",
    "    df_mmr['Gb(T|X,S,A)'] = df_mmr.apply(lambda r:\\\n",
    "        eval_surv_(Gb_C[f\"t_S{int(r['S'])}_A{int(r['A'])}\"], Gb_C[f\"St_S{int(r['S'])}_A{int(r['A'])}\"], r['T']), axis=1)\n",
    "\n",
    "    ipw_est(df_mmr, S=0, baseline='impute')  # censored observations are IMPUTED\n",
    "    ipw_est(df_mmr, S=1, baseline='impute')  # censored observations are IMPUTED\n",
    "\n",
    "    ipcw_est(df_mmr, S=0)\n",
    "    ipcw_est(df_mmr, S=1)\n",
    "\n",
    "    dr_est(df_mmr, S=0, baseline='impute')  # censored observations are IMPUTED\n",
    "    dr_est(df_mmr, S=1, baseline='impute')  # censored observations are IMPUTED\n",
    "\n",
    "    cdr_est(df_mmr, jD['cov_list'], Gb_C, Fb_Y, S=0, mis_spec='None')  \n",
    "    cdr_est(df_mmr, jD['cov_list'], Gb_C, Fb_Y, S=1, mis_spec='None')  \n",
    "\n",
    "    mmr_stats = np.zeros((len(mmr_keys), 2))\n",
    "\n",
    "    # run mmr test and record the results\n",
    "    for kind, key in enumerate(mmr_keys):\n",
    "        signal0, signal1 = jD['test_signals'][key][0], jD['test_signals'][key][1]\n",
    "        mmr_stats[kind, 0], mmr_stats[kind, 1] =\\\n",
    "                mmr_test(df_mmr, jD['cov_list'], jD['B'], laplacian_kernel, signal0, signal1)\n",
    "        \n",
    "    return mmr_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.read_csv(\"/data/whi/data/main_study/processed/whi_combined.csv\")  # combined dataset  # combined dataset\n",
    "orig_cov_list = list(df_combined.columns[4:])  # original set of covariates\n",
    "print(f'Original data size: {len(df_combined)}') \n",
    "\n",
    "mmr_keys = [\"IPW-Impute\", \"DR-Impute\", \"IPCW\", \"CDR\"]  # The signals to run the falsification test with\n",
    "jD = read_json_whi('whi/whi.json', mmr_keys)  # read the experiment settings\n",
    "\n",
    "PC_dim_list = [1, 10, 100, 200, 500]  # num. Principal components to experiment with \n",
    "num_folds = 10  # we train the models using 9 folds and run the MMR test with the predictions in the remaining fold\n",
    "\n",
    "mmr_global_stats = np.zeros((len(PC_dim_list), num_folds, len(mmr_keys), 2))  # store results and p-val for each mmr test\n",
    "\n",
    "for pcind, PC_dim in enumerate(PC_dim_list):\n",
    "    t1 = time()\n",
    "    df_pca = df_combined.copy()\n",
    "    df_pca = df_pca.sample(frac=1, random_state=1337)  # shuffle the rows so that S=0 and S=1 are not stacked\n",
    "\n",
    "    pca = PCA(n_components=PC_dim)\n",
    "    principalComponents = pca.fit_transform(df_pca[orig_cov_list])\n",
    "    print(f\"Var. explained by {PC_dim} PCs: {sum(pca.explained_variance_ratio_):.2f}\")\n",
    "\n",
    "    df_pca_new_cols = pd.DataFrame(data=sm.add_constant(principalComponents, prepend=True), columns=[f\"PC{i}\" for i in range(PC_dim + 1)])\n",
    "    df_pca = df_pca.drop(orig_cov_list, axis=1)\n",
    "    df_pca = pd.concat([df_pca, df_pca_new_cols], axis=1)\n",
    "\n",
    "    jD[\"cov_list\"] = [f\"PC{i}\" for i in range(PC_dim + 1)]\n",
    "    kf = KFold(n_splits=num_folds)\n",
    "    \n",
    "    local_mmr_res = Parallel(n_jobs=20)(delayed(run_single_mmr)(df_pca.copy(), train_index, mmr_index, mmr_keys.copy(), jD.copy()) \\\n",
    "                                 for (train_index, mmr_index) in kf.split(df_pca))\n",
    "\n",
    "    # run mmr test and record the results\n",
    "    for iter in range(num_folds):\n",
    "        for kind, key in enumerate(mmr_keys):\n",
    "            mmr_global_stats[pcind, iter, kind, 0] = local_mmr_res[iter][kind][0]\n",
    "            mmr_global_stats[pcind, iter, kind, 1] = local_mmr_res[iter][kind][1]\n",
    "\n",
    "    time_elapsed = time() - t1\n",
    "    print(f\"PC: {PC_dim} completed in {time_elapsed:.1f} seconds\")\n",
    "\n",
    "pc_vals = [f\"PC {pc}\" for pc in PC_dim_list]\n",
    "\n",
    "avg_reject = np.mean(mmr_global_stats[:,:,:,0], axis=1)\n",
    "avg_pval = np.mean(mmr_global_stats[:,:,:,1], axis=1)\n",
    "\n",
    "reject_df = pd.DataFrame(avg_reject, columns=mmr_keys, index=pc_vals)\n",
    "pval_df = pd.DataFrame(avg_pval, columns=mmr_keys, index=pc_vals)\n",
    "\n",
    "reject_df.to_csv(f\"results/whi/reject_rates_{num_folds}.csv\")\n",
    "pval_df.to_csv(f\"results/whi/pvals_avg_{num_folds}.csv\")\n",
    "np.save(f\"results/whi/all_results_{num_folds}.npy\", mmr_global_stats) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reject_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_combined = pd.read_csv(\"data/whi/whi_combined.csv\")  # combined dataset\n",
    "# orig_cov_list = list(df_combined.columns[4:])  # original set of covariates\n",
    "# print(f'Original data size: {len(df_combined)}') \n",
    "\n",
    "# mmr_keys = [\"IPW-Impute\", \"DR-Impute\", \"IPCW\", \"CDR\"]  # The signals to run the falsification test with\n",
    "# jD = read_json_whi('whi/whi.json', mmr_keys)  # read the experiment settings\n",
    "\n",
    "# PC_dim_list = [10, 50, 100, 200]  # num. Principal components to experiment with \n",
    "# num_folds = 10  # we train the models using 9 folds and run the MMR test with the predictions in the remaining fold\n",
    "\n",
    "# mmr_stats = np.zeros((len(PC_dim_list), num_folds, len(mmr_keys), 2))  # store results and p-val for each mmr test\n",
    "\n",
    "# for pcind, PC_dim in enumerate(PC_dim_list):\n",
    "#     df_pca = df_combined.copy()\n",
    "#     df_pca = df_pca.sample(frac=1, random_state=42)  # shuffle the rows so that S=0 and S=1 are not stacked\n",
    "\n",
    "#     pca = PCA(n_components=PC_dim)\n",
    "#     principalComponents = pca.fit_transform(df_pca[orig_cov_list])\n",
    "#     print(f\"Var. explained by {PC_dim} PCs: {sum(pca.explained_variance_ratio_)}\")\n",
    "\n",
    "#     df_pca_new_cols = pd.DataFrame(data=sm.add_constant(principalComponents, prepend=True), columns=[f\"PC{i}\" for i in range(PC_dim + 1)])\n",
    "#     df_pca = df_pca.drop(orig_cov_list, axis=1)\n",
    "#     df_pca = pd.concat([df_pca, df_pca_new_cols], axis=1)\n",
    "\n",
    "#     jD[\"cov_list\"] = [f\"PC{i}\" for i in range(PC_dim + 1)]\n",
    "#     kf = KFold(n_splits=num_folds)\n",
    "\n",
    "    \n",
    "#     results = Parallel(n_jobs=2)(delayed(process)(i) for i in range(10))\n",
    "#     print(results)\n",
    "\n",
    "#     for iter, (train_index, mmr_index) in enumerate(kf.split(df_pca)):\n",
    "#         t1 = time()\n",
    "\n",
    "#         df_train = df_pca.iloc[train_index, :].copy()\n",
    "#         df_mmr = df_pca.iloc[mmr_index, :].copy()\n",
    "\n",
    "#         # fit a model for the selection score P(S=1|X) and clip the population based on it\n",
    "#         X, y = df_train[jD[\"cov_list\"]], df_train[\"S\"]\n",
    "\n",
    "#         logreg = LogisticRegressionCV(cv=5, random_state=42, penalty='l2', solver='lbfgs')\n",
    "#         logreg.fit(X, y)\n",
    "\n",
    "#         df_mmr['P(S=1|X)'] = logreg.predict_proba(df_mmr[jD[\"cov_list\"]])[:,1]\n",
    "#         df_mmr = df_mmr.loc[(df_mmr['P(S=1|X)'] > 0.05) \\\n",
    "#                             & (df_mmr['P(S=1|X)']< 0.95)].reset_index(drop=True)\n",
    "        \n",
    "#         # fit a model for the propensity scores P(A=1|X,S) and clip the population based on them\n",
    "#         for s in range(2):\n",
    "#             X = df_train.query(f\"S=={s}\")[jD[\"cov_list\"]]\n",
    "#             y = df_train.query(f\"S=={s}\")[\"A\"]\n",
    "\n",
    "#             logreg = LogisticRegressionCV(cv=5, random_state=42, penalty='l2', solver='lbfgs')\n",
    "#             logreg.fit(X, y)\n",
    "\n",
    "#             df_mmr.loc[df_pca['S']==s, 'P(A=1|X,S)'] = logreg.predict_proba(df_mmr[jD[\"cov_list\"]])[:,1]\n",
    "\n",
    "#         df_mmr = df_mmr.loc[(df_mmr['P(A=1|X,S)'] > 0.05) \\\n",
    "#                             & (df_mmr['P(A=1|X,S)']< 0.95)].reset_index(drop=True)\n",
    "\n",
    "#         if iter == 0:\n",
    "#             plt.figure()\n",
    "#             plt.hist(df_mmr['P(S=1|X)'], bins=100, label='P(S=1|X)', alpha=0.5)\n",
    "#             plt.hist(df_mmr['P(A=1|X,S)'], bins=100, label='P(A=1|X,S)', alpha=0.5)\n",
    "#             plt.legend()\n",
    "#             plt.title(f\"Num PC: {PC_dim}\")\n",
    "#             plt.show()\n",
    "\n",
    "#         # fit response surface signals with the imputed data\n",
    "#         mu_regressor = {}\n",
    "#         for s in range(2):\n",
    "#             for a in range(2):\n",
    "#                 mu_regressor[f'S{s}_A{a}'] =\\\n",
    "#                 mu_est_baseline(df_train.query(f'S=={s} & A=={a}').copy(), 'T', jD[\"cov_list\"])\n",
    "\n",
    "#             df_mmr.loc[df_mmr[\"S\"]==s, 'mu(Y|X,S,A=0)'] = mu_regressor[f'S{s}_A0'].predict(df_mmr.loc[df_mmr[\"S\"]==s, jD[\"cov_list\"]])\n",
    "#             df_mmr.loc[df_mmr[\"S\"]==s, 'mu(Y|X,S,A=1)'] = mu_regressor[f'S{s}_A1'].predict(df_mmr.loc[df_mmr[\"S\"]==s, jD[\"cov_list\"]])\n",
    "\n",
    "#         # fit the survival curves\n",
    "#         Gb_C, Fb_Y = est_surv_whi(df_train, 'coxph', jD[\"cov_list\"], downsample=10)\n",
    "#         df_mmr['Gb(T|X,S,A)'] = df_mmr.apply(lambda r:\\\n",
    "#             eval_surv_(Gb_C[f\"t_S{int(r['S'])}_A{int(r['A'])}\"], Gb_C[f\"St_S{int(r['S'])}_A{int(r['A'])}\"], r['T']), axis=1)\n",
    "\n",
    "#         ipw_est(df_mmr, S=0, baseline='impute')  # censored observations are IMPUTED\n",
    "#         ipw_est(df_mmr, S=1, baseline='impute')  # censored observations are IMPUTED\n",
    "\n",
    "#         ipcw_est(df_mmr, S=0)\n",
    "#         ipcw_est(df_mmr, S=1)\n",
    "\n",
    "#         dr_est(df_mmr, S=0, baseline='impute')  # censored observations are IMPUTED\n",
    "#         dr_est(df_mmr, S=1, baseline='impute')  # censored observations are IMPUTED\n",
    "\n",
    "#         cdr_est(df_mmr, jD['cov_list'], Gb_C, Fb_Y, S=0, mis_spec='None')  \n",
    "#         cdr_est(df_mmr, jD['cov_list'], Gb_C, Fb_Y, S=1, mis_spec='None')  \n",
    "\n",
    "#         # run mmr test and record the results\n",
    "#         for kind, key in enumerate(mmr_keys):\n",
    "#             signal0, signal1 = jD['test_signals'][key][0], jD['test_signals'][key][1]\n",
    "#             mmr_stats[pcind, iter, kind, 0], mmr_stats[pcind, iter, kind, 1] =\\\n",
    "#                   mmr_test(df_mmr, jD['cov_list'], jD['B'], laplacian_kernel, signal0, signal1)\n",
    "\n",
    "#         time_elapsed = time() - t1\n",
    "#         print(f\"PC: {PC_dim}, Iter:{iter}/{num_folds} completed in {time_elapsed:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cov_list = []\n",
    "# with open('data/whi/whi_features_new.txt', 'r') as f:\n",
    "#     cov_list = f.read().splitlines()\n",
    "\n",
    "# print(f\"Num covs: {len(cov_list)}\")\n",
    "\n",
    "# #X = df_combined[jD[\"cov_list\"]].drop(columns=['DMARM_Intervention', 'DMARM_Not randomized to DM', 'CADARM_Intervention', 'CADARM_Not randomized to DM'])\n",
    "# X = df_combined[cov_list]\n",
    "# y = df_combined[\"S\"]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# logreg = LogisticRegressionCV(cv=5, random_state=0, penalty='l2', solver='lbfgs', l1_ratios=[0.5])\n",
    "# logreg.fit(X, y)\n",
    "\n",
    "# y_train_pred = logreg.predict(X_train)\n",
    "# y_train_proba = logreg.predict_proba(X_train)\n",
    "\n",
    "# y_test_pred = logreg.predict(X_test)\n",
    "# y_test_proba = logreg.predict_proba(X_test)\n",
    "\n",
    "# train_acc = (y_train_pred == y_train).mean()\n",
    "# test_acc = (y_test_pred == y_test).mean()\n",
    "\n",
    "# print(f'Train acc :{train_acc}\\nTest acc : {test_acc}')\n",
    "\n",
    "# plt.hist(logreg.predict_proba(X)[:,1], bins=100)\n",
    "# plt.show()\n",
    "\n",
    "# n = 50\n",
    "\n",
    "# top_coef_indices = np.argsort(np.abs(logreg.coef_[0]))[::-1][:n]\n",
    "# top_coef_features = X.columns[top_coef_indices]\n",
    "# top_coef_values = logreg.coef_[0][top_coef_indices]\n",
    "\n",
    "# # Print the feature names and their coefficients\n",
    "# for feature, coef in zip(top_coef_features, top_coef_values):\n",
    "#     print(f'Coeff: {coef:.2f}, Ft: {feature}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a = 0, 0\n",
    "ty, sty = Fb_Y[f't_S{s}_A{a}'], Fb_Y[f'St_S{s}_A{a}']\n",
    "tc, stc = Gb_C[f't_S{s}_A{a}'], Gb_C[f'St_S{s}_A{a}']\n",
    "\n",
    "t_arr = ty[::10] #\n",
    "st_arr = sty[::10]\n",
    "\n",
    "t1 = time()\n",
    "func = interp1d(t_arr, st_arr, kind='nearest', fill_value='extrapolate')\n",
    "result, error = quad(func, 0, t_arr.max() + 10, limit=5)\n",
    "\n",
    "print(f\"Time: {time()-t1:.2f} s.\")\n",
    "print(f\"Result of integration: {result}, error: {error}\")\n",
    "\n",
    "xnew = np.arange(0, t_arr.max(), 0.1)\n",
    "ynew = func(xnew)   # use interpolation function returned by `interp1d`\n",
    "plt.plot(t_arr, st_arr, 'o', xnew, ynew, '--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whi_merged = pd.read_csv('/data/whi/data/main_study/processed/merged.csv')\n",
    "# whi_merged_orig = whi_merged.copy()\n",
    "\n",
    "# whi_merged = sm.add_constant(whi_merged, prepend=False)\n",
    "\n",
    "# whi_merged.rename(columns={'GLBL_DY': 'T', 'GLBL_E': 'Delta', 'OS': 'S', 'HRTARM': 'A'}, inplace=True)\n",
    "# whi_merged = whi_merged.drop(columns=[col for col in whi_merged.columns if any(s in col for s in ['_E', '_DY', 'ID']  )])\n",
    "\n",
    "# dummy_col_names = [col for col in whi_merged.columns if '_' in col]\n",
    "# prefixes = pd.Series([col.split('_')[0] for col in dummy_col_names]).unique()\n",
    "\n",
    "# for prefix in prefixes:\n",
    "#     cols = [col for col in whi_merged.columns if col.startswith(prefix + '_')]\n",
    "#     if len(cols) > 1:\n",
    "#         whi_merged = whi_merged.drop(sorted(cols)[0], axis=1)\n",
    "\n",
    "# new_col_order = [*whi_merged.columns]\n",
    "# last_elt = new_col_order.pop()\n",
    "# new_col_order.insert(4, last_elt)\n",
    "\n",
    "# whi_merged = whi_merged[new_col_order]\n",
    "# whi_merged.to_csv('/data/whi/data/main_study/processed/merged_v2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
