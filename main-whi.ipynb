{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from sklearn.metrics.pairwise import  laplacian_kernel\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy.integrate import quad\n",
    "from scipy.interpolate import interp1d\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from WHIDataModule import *\n",
    "from estimators import *\n",
    "from utils import *\n",
    "from mmr_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_mmr(df_pca, train_index, mmr_index, mmr_keys, jD):\n",
    "\n",
    "    df_train = df_pca.iloc[train_index, :].copy()\n",
    "    df_mmr = df_pca.iloc[mmr_index, :].copy()\n",
    "\n",
    "    # fit a model for the selection score P(S=1|X) and clip the population based on it\n",
    "    X, y = df_train[jD[\"cov_list\"]], df_train[\"S\"]\n",
    "\n",
    "    logreg = LogisticRegressionCV(cv=5, random_state=42, penalty='l2', solver='lbfgs')\n",
    "    logreg.fit(X, y)\n",
    "\n",
    "    df_mmr['P(S=1|X)'] = logreg.predict_proba(df_mmr[jD[\"cov_list\"]])[:,1]\n",
    "    df_mmr = df_mmr.loc[(df_mmr['P(S=1|X)'] > 0.05) \\\n",
    "                        & (df_mmr['P(S=1|X)']< 0.95)].reset_index(drop=True)\n",
    "\n",
    "    # fit a model for the propensity scores P(A=1|X,S) and clip the population based on them\n",
    "    for s in range(2):\n",
    "        X = df_train.query(f\"S=={s}\")[jD[\"cov_list\"]]\n",
    "        y = df_train.query(f\"S=={s}\")[\"A\"]\n",
    "\n",
    "        logreg = LogisticRegressionCV(cv=5, random_state=42, penalty='l2', solver='lbfgs')\n",
    "        logreg.fit(X, y)\n",
    "\n",
    "        df_mmr.loc[df_pca['S']==s, 'P(A=1|X,S)'] = logreg.predict_proba(df_mmr[jD[\"cov_list\"]])[:,1]\n",
    "\n",
    "    df_mmr = df_mmr.loc[(df_mmr['P(A=1|X,S)'] > 0.05) \\\n",
    "                        & (df_mmr['P(A=1|X,S)']< 0.95)].reset_index(drop=True)\n",
    "\n",
    "    # fit response surface signals with the imputed data\n",
    "    mu_regressor = {}\n",
    "    for s in range(2):\n",
    "        for a in range(2):\n",
    "            mu_regressor[f'S{s}_A{a}'] =\\\n",
    "            mu_est_baseline(df_train.query(f'S=={s} & A=={a}').copy(), 'T', jD[\"cov_list\"], model_name='linear')\n",
    "\n",
    "        df_mmr.loc[df_mmr[\"S\"]==s, 'mu(Y|X,S,A=0)'] = mu_regressor[f'S{s}_A0'].predict(df_mmr.loc[df_mmr[\"S\"]==s, jD[\"cov_list\"]])\n",
    "        df_mmr.loc[df_mmr[\"S\"]==s, 'mu(Y|X,S,A=1)'] = mu_regressor[f'S{s}_A1'].predict(df_mmr.loc[df_mmr[\"S\"]==s, jD[\"cov_list\"]])\n",
    "\n",
    "    # fit the survival curves\n",
    "    Gb_C, Fb_Y = est_surv_whi(df_train, 'coxph', jD[\"cov_list\"], downsample=10)\n",
    "    df_mmr['Gb(T|X,S,A)'] = df_mmr.apply(lambda r:\\\n",
    "        eval_surv_(Gb_C[f\"t_S{int(r['S'])}_A{int(r['A'])}\"], Gb_C[f\"St_S{int(r['S'])}_A{int(r['A'])}\"], r['T']), axis=1)\n",
    "\n",
    "    ipw_est(df_mmr, S=0, baseline='impute')  # censored observations are IMPUTED\n",
    "    ipw_est(df_mmr, S=1, baseline='impute')  # censored observations are IMPUTED\n",
    "\n",
    "    ipcw_est(df_mmr, S=0)\n",
    "    ipcw_est(df_mmr, S=1)\n",
    "\n",
    "    dr_est(df_mmr, S=0, baseline='impute')  # censored observations are IMPUTED\n",
    "    dr_est(df_mmr, S=1, baseline='impute')  # censored observations are IMPUTED\n",
    "\n",
    "    cdr_est(df_mmr, jD['cov_list'], Gb_C, Fb_Y, S=0, mis_spec='None')  \n",
    "    cdr_est(df_mmr, jD['cov_list'], Gb_C, Fb_Y, S=1, mis_spec='None')  \n",
    "\n",
    "    mmr_stats = np.zeros((len(mmr_keys), 2))\n",
    "\n",
    "    # run mmr test and record the results\n",
    "    for kind, key in enumerate(mmr_keys):\n",
    "        signal0, signal1 = jD['test_signals'][key][0], jD['test_signals'][key][1]\n",
    "        mmr_stats[kind, 0], mmr_stats[kind, 1] =\\\n",
    "                mmr_test(df_mmr, jD['cov_list'], jD['B'], laplacian_kernel, signal0, signal1)\n",
    "        \n",
    "    return mmr_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.read_csv(\"/data/whi/data/main_study/processed/whi_combined.csv\")  # combined dataset  # combined dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(df_combined.query(\"Delta == 0\")[\"T\"], bins=100)\n",
    "# plt.show()\n",
    "\n",
    "df_combined = df_combined[~((df_combined[\"Delta\"] == 0) & (df_combined[\"T\"] == 2555))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = df_combined.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data size: 19231\n",
      "Var. explained by 1 PCs: 0.07\n",
      "PC: 1 completed in 60.5 seconds\n",
      "Var. explained by 10 PCs: 0.27\n",
      "PC: 10 completed in 58.0 seconds\n",
      "Var. explained by 100 PCs: 0.64\n",
      "PC: 100 completed in 88.1 seconds\n",
      "Var. explained by 200 PCs: 0.78\n",
      "PC: 200 completed in 132.4 seconds\n",
      "Var. explained by 500 PCs: 0.96\n",
      "PC: 500 completed in 443.6 seconds\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'results/whi'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/demirel/MMR-censored/censored-mmr-synthetic/main-whi.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhera/home/demirel/MMR-censored/censored-mmr-synthetic/main-whi.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m reject_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(avg_reject, columns\u001b[39m=\u001b[39mmmr_keys, index\u001b[39m=\u001b[39mpc_vals)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhera/home/demirel/MMR-censored/censored-mmr-synthetic/main-whi.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m pval_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(avg_pval, columns\u001b[39m=\u001b[39mmmr_keys, index\u001b[39m=\u001b[39mpc_vals)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bhera/home/demirel/MMR-censored/censored-mmr-synthetic/main-whi.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m reject_df\u001b[39m.\u001b[39mto_csv(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mresults/whi/reject_rates_\u001b[39m\u001b[39m{\u001b[39;00mnum_folds\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhera/home/demirel/MMR-censored/censored-mmr-synthetic/main-whi.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m pval_df\u001b[39m.\u001b[39mto_csv(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mresults/whi/pvals_avg_\u001b[39m\u001b[39m{\u001b[39;00mnum_folds\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhera/home/demirel/MMR-censored/censored-mmr-synthetic/main-whi.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m np\u001b[39m.\u001b[39msave(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mresults/whi/all_results_\u001b[39m\u001b[39m{\u001b[39;00mnum_folds\u001b[39m}\u001b[39;00m\u001b[39m.npy\u001b[39m\u001b[39m\"\u001b[39m, mmr_global_stats)\n",
      "File \u001b[0;32m/opt/conda/demirel/envs/cenfal/lib/python3.11/site-packages/pandas/core/generic.py:3772\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3761\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ABCDataFrame) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_frame()\n\u001b[1;32m   3763\u001b[0m formatter \u001b[39m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3764\u001b[0m     frame\u001b[39m=\u001b[39mdf,\n\u001b[1;32m   3765\u001b[0m     header\u001b[39m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3769\u001b[0m     decimal\u001b[39m=\u001b[39mdecimal,\n\u001b[1;32m   3770\u001b[0m )\n\u001b[0;32m-> 3772\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[39m.\u001b[39mto_csv(\n\u001b[1;32m   3773\u001b[0m     path_or_buf,\n\u001b[1;32m   3774\u001b[0m     lineterminator\u001b[39m=\u001b[39mlineterminator,\n\u001b[1;32m   3775\u001b[0m     sep\u001b[39m=\u001b[39msep,\n\u001b[1;32m   3776\u001b[0m     encoding\u001b[39m=\u001b[39mencoding,\n\u001b[1;32m   3777\u001b[0m     errors\u001b[39m=\u001b[39merrors,\n\u001b[1;32m   3778\u001b[0m     compression\u001b[39m=\u001b[39mcompression,\n\u001b[1;32m   3779\u001b[0m     quoting\u001b[39m=\u001b[39mquoting,\n\u001b[1;32m   3780\u001b[0m     columns\u001b[39m=\u001b[39mcolumns,\n\u001b[1;32m   3781\u001b[0m     index_label\u001b[39m=\u001b[39mindex_label,\n\u001b[1;32m   3782\u001b[0m     mode\u001b[39m=\u001b[39mmode,\n\u001b[1;32m   3783\u001b[0m     chunksize\u001b[39m=\u001b[39mchunksize,\n\u001b[1;32m   3784\u001b[0m     quotechar\u001b[39m=\u001b[39mquotechar,\n\u001b[1;32m   3785\u001b[0m     date_format\u001b[39m=\u001b[39mdate_format,\n\u001b[1;32m   3786\u001b[0m     doublequote\u001b[39m=\u001b[39mdoublequote,\n\u001b[1;32m   3787\u001b[0m     escapechar\u001b[39m=\u001b[39mescapechar,\n\u001b[1;32m   3788\u001b[0m     storage_options\u001b[39m=\u001b[39mstorage_options,\n\u001b[1;32m   3789\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/demirel/envs/cenfal/lib/python3.11/site-packages/pandas/io/formats/format.py:1186\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1165\u001b[0m     created_buffer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m csv_formatter \u001b[39m=\u001b[39m CSVFormatter(\n\u001b[1;32m   1168\u001b[0m     path_or_buf\u001b[39m=\u001b[39mpath_or_buf,\n\u001b[1;32m   1169\u001b[0m     lineterminator\u001b[39m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1184\u001b[0m     formatter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmt,\n\u001b[1;32m   1185\u001b[0m )\n\u001b[0;32m-> 1186\u001b[0m csv_formatter\u001b[39m.\u001b[39msave()\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1189\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m/opt/conda/demirel/envs/cenfal/lib/python3.11/site-packages/pandas/io/formats/csvs.py:240\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[39mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[39m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[1;32m    241\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilepath_or_buffer,\n\u001b[1;32m    242\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode,\n\u001b[1;32m    243\u001b[0m     encoding\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding,\n\u001b[1;32m    244\u001b[0m     errors\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrors,\n\u001b[1;32m    245\u001b[0m     compression\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompression,\n\u001b[1;32m    246\u001b[0m     storage_options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstorage_options,\n\u001b[1;32m    247\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[1;32m    248\u001b[0m     \u001b[39m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m csvlib\u001b[39m.\u001b[39mwriter(\n\u001b[1;32m    250\u001b[0m         handles\u001b[39m.\u001b[39mhandle,\n\u001b[1;32m    251\u001b[0m         lineterminator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    256\u001b[0m         quotechar\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquotechar,\n\u001b[1;32m    257\u001b[0m     )\n\u001b[1;32m    259\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save()\n",
      "File \u001b[0;32m/opt/conda/demirel/envs/cenfal/lib/python3.11/site-packages/pandas/io/common.py:737\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[39m# Only for write methods\u001b[39;00m\n\u001b[1;32m    736\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode \u001b[39mand\u001b[39;00m is_path:\n\u001b[0;32m--> 737\u001b[0m     check_parent_directory(\u001b[39mstr\u001b[39m(handle))\n\u001b[1;32m    739\u001b[0m \u001b[39mif\u001b[39;00m compression:\n\u001b[1;32m    740\u001b[0m     \u001b[39mif\u001b[39;00m compression \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzstd\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    741\u001b[0m         \u001b[39m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/demirel/envs/cenfal/lib/python3.11/site-packages/pandas/io/common.py:600\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    598\u001b[0m parent \u001b[39m=\u001b[39m Path(path)\u001b[39m.\u001b[39mparent\n\u001b[1;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m parent\u001b[39m.\u001b[39mis_dir():\n\u001b[0;32m--> 600\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39mrf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot save file into a non-existent directory: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mparent\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'results/whi'"
     ]
    }
   ],
   "source": [
    "# df_combined = pd.read_csv(\"/data/whi/data/main_study/processed/whi_combined.csv\")  # combined dataset  # combined dataset\n",
    "orig_cov_list = list(df_combined.columns[4:])  # original set of covariates\n",
    "print(f'Original data size: {len(df_combined)}') \n",
    "\n",
    "mmr_keys = [\"IPW-Impute\", \"DR-Impute\", \"IPCW\", \"CDR\"]  # The signals to run the falsification test with\n",
    "jD = read_json_whi('whi/whi.json', mmr_keys)  # read the experiment settings\n",
    "\n",
    "PC_dim_list = [1, 10, 100, 200, 500]  # num. Principal components to experiment with \n",
    "num_folds = 5  # we train the models using 9 folds and run the MMR test with the predictions in the remaining fold\n",
    "\n",
    "mmr_global_stats = np.zeros((len(PC_dim_list), num_folds, len(mmr_keys), 2))  # store results and p-val for each mmr test\n",
    "\n",
    "for pcind, PC_dim in enumerate(PC_dim_list):\n",
    "    t1 = time()\n",
    "    df_pca = df_combined.copy()\n",
    "    df_pca = df_pca.sample(frac=1, random_state=1337)  # shuffle the rows so that S=0 and S=1 are not stacked\n",
    "\n",
    "    pca = PCA(n_components=PC_dim)\n",
    "    principalComponents = pca.fit_transform(df_pca[orig_cov_list])\n",
    "    print(f\"Var. explained by {PC_dim} PCs: {sum(pca.explained_variance_ratio_):.2f}\")\n",
    "\n",
    "    df_pca_new_cols = pd.DataFrame(data=sm.add_constant(principalComponents, prepend=True), columns=[f\"PC{i}\" for i in range(PC_dim + 1)])\n",
    "    df_pca = df_pca.drop(orig_cov_list, axis=1)\n",
    "    df_pca = pd.concat([df_pca, df_pca_new_cols], axis=1)\n",
    "\n",
    "    jD[\"cov_list\"] = [f\"PC{i}\" for i in range(PC_dim + 1)]\n",
    "    kf = KFold(n_splits=num_folds)\n",
    "    \n",
    "    local_mmr_res = Parallel(n_jobs=20)(delayed(run_single_mmr)(df_pca.copy(), train_index, mmr_index, mmr_keys.copy(), jD.copy()) \\\n",
    "                                 for (train_index, mmr_index) in kf.split(df_pca))\n",
    "\n",
    "    # run mmr test and record the results\n",
    "    for iter in range(num_folds):\n",
    "        for kind, key in enumerate(mmr_keys):\n",
    "            mmr_global_stats[pcind, iter, kind, 0] = local_mmr_res[iter][kind][0]\n",
    "            mmr_global_stats[pcind, iter, kind, 1] = local_mmr_res[iter][kind][1]\n",
    "\n",
    "    time_elapsed = time() - t1\n",
    "    print(f\"PC: {PC_dim} completed in {time_elapsed:.1f} seconds\")\n",
    "\n",
    "pc_vals = [f\"PC {pc}\" for pc in PC_dim_list]\n",
    "\n",
    "avg_reject = np.mean(mmr_global_stats[:,:,:,0], axis=1)\n",
    "avg_pval = np.mean(mmr_global_stats[:,:,:,1], axis=1)\n",
    "\n",
    "reject_df = pd.DataFrame(avg_reject, columns=mmr_keys, index=pc_vals)\n",
    "pval_df = pd.DataFrame(avg_pval, columns=mmr_keys, index=pc_vals)\n",
    "\n",
    "reject_df.to_csv(f\"results/whi/reject_rates_{num_folds}.csv\")\n",
    "pval_df.to_csv(f\"results/whi/pvals_avg_{num_folds}.csv\")\n",
    "np.save(f\"results/whi/all_results_{num_folds}.npy\", mmr_global_stats) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_vals = [f\"PC {pc}\" for pc in PC_dim_list]\n",
    "\n",
    "avg_reject = np.mean(mmr_global_stats[:,:,:,0], axis=1)\n",
    "avg_pval = np.mean(mmr_global_stats[:,:,:,1], axis=1)\n",
    "\n",
    "reject_df = pd.DataFrame(avg_reject, columns=mmr_keys, index=pc_vals)\n",
    "pval_df = pd.DataFrame(avg_pval, columns=mmr_keys, index=pc_vals)\n",
    "\n",
    "reject_df.to_csv(f\"results/whi/reject_rates_{num_folds}.csv\")\n",
    "pval_df.to_csv(f\"results/whi/pvals_avg_{num_folds}.csv\")\n",
    "np.save(f\"results/whi/all_results_{num_folds}.npy\", mmr_global_stats) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reject_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_combined = pd.read_csv(\"data/whi/whi_combined.csv\")  # combined dataset\n",
    "# orig_cov_list = list(df_combined.columns[4:])  # original set of covariates\n",
    "# print(f'Original data size: {len(df_combined)}') \n",
    "\n",
    "# mmr_keys = [\"IPW-Impute\", \"DR-Impute\", \"IPCW\", \"CDR\"]  # The signals to run the falsification test with\n",
    "# jD = read_json_whi('whi/whi.json', mmr_keys)  # read the experiment settings\n",
    "\n",
    "# PC_dim_list = [10, 50, 100, 200]  # num. Principal components to experiment with \n",
    "# num_folds = 10  # we train the models using 9 folds and run the MMR test with the predictions in the remaining fold\n",
    "\n",
    "# mmr_stats = np.zeros((len(PC_dim_list), num_folds, len(mmr_keys), 2))  # store results and p-val for each mmr test\n",
    "\n",
    "# for pcind, PC_dim in enumerate(PC_dim_list):\n",
    "#     df_pca = df_combined.copy()\n",
    "#     df_pca = df_pca.sample(frac=1, random_state=42)  # shuffle the rows so that S=0 and S=1 are not stacked\n",
    "\n",
    "#     pca = PCA(n_components=PC_dim)\n",
    "#     principalComponents = pca.fit_transform(df_pca[orig_cov_list])\n",
    "#     print(f\"Var. explained by {PC_dim} PCs: {sum(pca.explained_variance_ratio_)}\")\n",
    "\n",
    "#     df_pca_new_cols = pd.DataFrame(data=sm.add_constant(principalComponents, prepend=True), columns=[f\"PC{i}\" for i in range(PC_dim + 1)])\n",
    "#     df_pca = df_pca.drop(orig_cov_list, axis=1)\n",
    "#     df_pca = pd.concat([df_pca, df_pca_new_cols], axis=1)\n",
    "\n",
    "#     jD[\"cov_list\"] = [f\"PC{i}\" for i in range(PC_dim + 1)]\n",
    "#     kf = KFold(n_splits=num_folds)\n",
    "\n",
    "    \n",
    "#     results = Parallel(n_jobs=2)(delayed(process)(i) for i in range(10))\n",
    "#     print(results)\n",
    "\n",
    "#     for iter, (train_index, mmr_index) in enumerate(kf.split(df_pca)):\n",
    "#         t1 = time()\n",
    "\n",
    "#         df_train = df_pca.iloc[train_index, :].copy()\n",
    "#         df_mmr = df_pca.iloc[mmr_index, :].copy()\n",
    "\n",
    "#         # fit a model for the selection score P(S=1|X) and clip the population based on it\n",
    "#         X, y = df_train[jD[\"cov_list\"]], df_train[\"S\"]\n",
    "\n",
    "#         logreg = LogisticRegressionCV(cv=5, random_state=42, penalty='l2', solver='lbfgs')\n",
    "#         logreg.fit(X, y)\n",
    "\n",
    "#         df_mmr['P(S=1|X)'] = logreg.predict_proba(df_mmr[jD[\"cov_list\"]])[:,1]\n",
    "#         df_mmr = df_mmr.loc[(df_mmr['P(S=1|X)'] > 0.05) \\\n",
    "#                             & (df_mmr['P(S=1|X)']< 0.95)].reset_index(drop=True)\n",
    "        \n",
    "#         # fit a model for the propensity scores P(A=1|X,S) and clip the population based on them\n",
    "#         for s in range(2):\n",
    "#             X = df_train.query(f\"S=={s}\")[jD[\"cov_list\"]]\n",
    "#             y = df_train.query(f\"S=={s}\")[\"A\"]\n",
    "\n",
    "#             logreg = LogisticRegressionCV(cv=5, random_state=42, penalty='l2', solver='lbfgs')\n",
    "#             logreg.fit(X, y)\n",
    "\n",
    "#             df_mmr.loc[df_pca['S']==s, 'P(A=1|X,S)'] = logreg.predict_proba(df_mmr[jD[\"cov_list\"]])[:,1]\n",
    "\n",
    "#         df_mmr = df_mmr.loc[(df_mmr['P(A=1|X,S)'] > 0.05) \\\n",
    "#                             & (df_mmr['P(A=1|X,S)']< 0.95)].reset_index(drop=True)\n",
    "\n",
    "#         if iter == 0:\n",
    "#             plt.figure()\n",
    "#             plt.hist(df_mmr['P(S=1|X)'], bins=100, label='P(S=1|X)', alpha=0.5)\n",
    "#             plt.hist(df_mmr['P(A=1|X,S)'], bins=100, label='P(A=1|X,S)', alpha=0.5)\n",
    "#             plt.legend()\n",
    "#             plt.title(f\"Num PC: {PC_dim}\")\n",
    "#             plt.show()\n",
    "\n",
    "#         # fit response surface signals with the imputed data\n",
    "#         mu_regressor = {}\n",
    "#         for s in range(2):\n",
    "#             for a in range(2):\n",
    "#                 mu_regressor[f'S{s}_A{a}'] =\\\n",
    "#                 mu_est_baseline(df_train.query(f'S=={s} & A=={a}').copy(), 'T', jD[\"cov_list\"])\n",
    "\n",
    "#             df_mmr.loc[df_mmr[\"S\"]==s, 'mu(Y|X,S,A=0)'] = mu_regressor[f'S{s}_A0'].predict(df_mmr.loc[df_mmr[\"S\"]==s, jD[\"cov_list\"]])\n",
    "#             df_mmr.loc[df_mmr[\"S\"]==s, 'mu(Y|X,S,A=1)'] = mu_regressor[f'S{s}_A1'].predict(df_mmr.loc[df_mmr[\"S\"]==s, jD[\"cov_list\"]])\n",
    "\n",
    "#         # fit the survival curves\n",
    "#         Gb_C, Fb_Y = est_surv_whi(df_train, 'coxph', jD[\"cov_list\"], downsample=10)\n",
    "#         df_mmr['Gb(T|X,S,A)'] = df_mmr.apply(lambda r:\\\n",
    "#             eval_surv_(Gb_C[f\"t_S{int(r['S'])}_A{int(r['A'])}\"], Gb_C[f\"St_S{int(r['S'])}_A{int(r['A'])}\"], r['T']), axis=1)\n",
    "\n",
    "#         ipw_est(df_mmr, S=0, baseline='impute')  # censored observations are IMPUTED\n",
    "#         ipw_est(df_mmr, S=1, baseline='impute')  # censored observations are IMPUTED\n",
    "\n",
    "#         ipcw_est(df_mmr, S=0)\n",
    "#         ipcw_est(df_mmr, S=1)\n",
    "\n",
    "#         dr_est(df_mmr, S=0, baseline='impute')  # censored observations are IMPUTED\n",
    "#         dr_est(df_mmr, S=1, baseline='impute')  # censored observations are IMPUTED\n",
    "\n",
    "#         cdr_est(df_mmr, jD['cov_list'], Gb_C, Fb_Y, S=0, mis_spec='None')  \n",
    "#         cdr_est(df_mmr, jD['cov_list'], Gb_C, Fb_Y, S=1, mis_spec='None')  \n",
    "\n",
    "#         # run mmr test and record the results\n",
    "#         for kind, key in enumerate(mmr_keys):\n",
    "#             signal0, signal1 = jD['test_signals'][key][0], jD['test_signals'][key][1]\n",
    "#             mmr_stats[pcind, iter, kind, 0], mmr_stats[pcind, iter, kind, 1] =\\\n",
    "#                   mmr_test(df_mmr, jD['cov_list'], jD['B'], laplacian_kernel, signal0, signal1)\n",
    "\n",
    "#         time_elapsed = time() - t1\n",
    "#         print(f\"PC: {PC_dim}, Iter:{iter}/{num_folds} completed in {time_elapsed:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cov_list = []\n",
    "# with open('data/whi/whi_features_new.txt', 'r') as f:\n",
    "#     cov_list = f.read().splitlines()\n",
    "\n",
    "# print(f\"Num covs: {len(cov_list)}\")\n",
    "\n",
    "# #X = df_combined[jD[\"cov_list\"]].drop(columns=['DMARM_Intervention', 'DMARM_Not randomized to DM', 'CADARM_Intervention', 'CADARM_Not randomized to DM'])\n",
    "# X = df_combined[cov_list]\n",
    "# y = df_combined[\"S\"]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# logreg = LogisticRegressionCV(cv=5, random_state=0, penalty='l2', solver='lbfgs', l1_ratios=[0.5])\n",
    "# logreg.fit(X, y)\n",
    "\n",
    "# y_train_pred = logreg.predict(X_train)\n",
    "# y_train_proba = logreg.predict_proba(X_train)\n",
    "\n",
    "# y_test_pred = logreg.predict(X_test)\n",
    "# y_test_proba = logreg.predict_proba(X_test)\n",
    "\n",
    "# train_acc = (y_train_pred == y_train).mean()\n",
    "# test_acc = (y_test_pred == y_test).mean()\n",
    "\n",
    "# print(f'Train acc :{train_acc}\\nTest acc : {test_acc}')\n",
    "\n",
    "# plt.hist(logreg.predict_proba(X)[:,1], bins=100)\n",
    "# plt.show()\n",
    "\n",
    "# n = 50\n",
    "\n",
    "# top_coef_indices = np.argsort(np.abs(logreg.coef_[0]))[::-1][:n]\n",
    "# top_coef_features = X.columns[top_coef_indices]\n",
    "# top_coef_values = logreg.coef_[0][top_coef_indices]\n",
    "\n",
    "# # Print the feature names and their coefficients\n",
    "# for feature, coef in zip(top_coef_features, top_coef_values):\n",
    "#     print(f'Coeff: {coef:.2f}, Ft: {feature}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a = 0, 0\n",
    "ty, sty = Fb_Y[f't_S{s}_A{a}'], Fb_Y[f'St_S{s}_A{a}']\n",
    "tc, stc = Gb_C[f't_S{s}_A{a}'], Gb_C[f'St_S{s}_A{a}']\n",
    "\n",
    "t_arr = ty[::10] #\n",
    "st_arr = sty[::10]\n",
    "\n",
    "t1 = time()\n",
    "func = interp1d(t_arr, st_arr, kind='nearest', fill_value='extrapolate')\n",
    "result, error = quad(func, 0, t_arr.max() + 10, limit=5)\n",
    "\n",
    "print(f\"Time: {time()-t1:.2f} s.\")\n",
    "print(f\"Result of integration: {result}, error: {error}\")\n",
    "\n",
    "xnew = np.arange(0, t_arr.max(), 0.1)\n",
    "ynew = func(xnew)   # use interpolation function returned by `interp1d`\n",
    "plt.plot(t_arr, st_arr, 'o', xnew, ynew, '--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whi_merged = pd.read_csv('/data/whi/data/main_study/processed/merged.csv')\n",
    "# whi_merged_orig = whi_merged.copy()\n",
    "\n",
    "# whi_merged = sm.add_constant(whi_merged, prepend=False)\n",
    "\n",
    "# whi_merged.rename(columns={'GLBL_DY': 'T', 'GLBL_E': 'Delta', 'OS': 'S', 'HRTARM': 'A'}, inplace=True)\n",
    "# whi_merged = whi_merged.drop(columns=[col for col in whi_merged.columns if any(s in col for s in ['_E', '_DY', 'ID']  )])\n",
    "\n",
    "# dummy_col_names = [col for col in whi_merged.columns if '_' in col]\n",
    "# prefixes = pd.Series([col.split('_')[0] for col in dummy_col_names]).unique()\n",
    "\n",
    "# for prefix in prefixes:\n",
    "#     cols = [col for col in whi_merged.columns if col.startswith(prefix + '_')]\n",
    "#     if len(cols) > 1:\n",
    "#         whi_merged = whi_merged.drop(sorted(cols)[0], axis=1)\n",
    "\n",
    "# new_col_order = [*whi_merged.columns]\n",
    "# last_elt = new_col_order.pop()\n",
    "# new_col_order.insert(4, last_elt)\n",
    "\n",
    "# whi_merged = whi_merged[new_col_order]\n",
    "# whi_merged.to_csv('/data/whi/data/main_study/processed/merged_v2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
